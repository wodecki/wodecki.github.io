[["wprowadzenie.html", "Architektury i metodyki wdrożeń systemów Sztucznej Inteligencji Rozdział 1 Wprowadzenie", " Architektury i metodyki wdrożeń systemów Sztucznej Inteligencji Andrzej Wodecki 2022-05-05 Rozdział 1 Wprowadzenie "],["motywacja.html", "1.1 Motywacja", " 1.1 Motywacja Trening modelu uczenia maszynowego to dopiero początek: wcześniej trzeba pozyskać i przygotować dane, następnie go udostępnić, a po udostępnieniu monitorować jego jakość. image-20220329115350377 W tym „cyklu życia modelu” pracujemy z wieloma komponentami i artefaktami: Komponenty „przetwarzają” dane, są najczęściej skryptami komputerowymi (lub modułami dostępnymi w formie kontenerów) Artefakty są produktami i/lub materiałem wejściowym dla komponentów Przykładowe komponenty to: przygotowanie danych, modelowanie, udostępnianie, monitoring, dotrenowanie… Przykładowe artefakty zas to: dane, modele, aplikacje Komponenty też nie są proste… Składa się na nie najczęściej wiele różnych, złożonych działań. Artefakty zaś wymagają składowania w dedykowanych strukturach, takich jak: Repozytoria cech (features stores) Repozytoria modeli (model stores) Repozytoria metadanych (metadata stores) Całość trzeba przy tym: monitorować: Jakość i charakterystyki danych (np. dryf danych) Wyniki eksperymentów Jakość modeli (np. dryf modeli) … kontrolować Kontrola przebiegu procesu Zarządzanie parametrami przebiegów… i wersjonować Dane Modele. Dane zadanie/proces ML może być realizowane: Na różne sposoby Z wykorzystaniem różnych bibliotek i różnych narzędzi W różnych środowiskach (lokalnie, serwery w chmurze, chmura bezserwerowa, urządzenia mobilne. image-20220329115417586 Źródło: https://ai-infrastructure.org/maximizing-ml-infrastructure-tools-for-production-workloads-arize-ai/ Na szczęście, pomaga nam w tym wiele różnych bibliotek i narzędzi. Krajobraz ten szybko się zmienia. "],["cele-kursu.html", "1.2 Cele kursu", " 1.2 Cele kursu Co zatem warto wiedzieć? Potrafić? O wiele ważniejsza od znajomości konkretnych narzędzi i technik jest wiedza o tym, co robić (w danej sytuacji), a nie jak to robić. Świadomość najważniejszych, w miarę uniwersalnych etapów, czynności, metod i dobrych praktyk (np. MLOps). I zdolność ich zastosowania w praktyce. Podczas tego kursu: Stworzysz projekt prostej architektury rozwiązania uczenia maszynowego uwzględniającego najważniejsze etapy cyklu życia modelu Zaimplementujesz tę architekturę w wybranym środowisku wykorzystując różne, dedykowane biblioteki Python Stworzysz dokumentację i plan uruchomienia takiego projektu z wykorzystaniem dobrych praktyk metodyki MLOps. "],["warsztaty.html", "1.3 Warsztaty", " 1.3 Warsztaty Pracę na ćwiczeniach podzielimy na 5 etapów: Stworzenie komponentów umożliwiających trening uruchomienie i monitoring modeli Implementację tych komponentów w postaci kontenerów Docker (opcjonalnie: Kubernetes lub Dataiku) Opracowanie dokumentacji całej architektury. W pracach wykorzystamy następujące narzędzia: Pakiet MLFlow Środowisko Docker/Kubeflow Platformę Dataiku. Na końcową dokumentację złożą się: Opis problemu: Co i dlaczego modelujemy? Analiza potrzeb. Krótka charakterystyka organizacji, dla której trenujemy model (1 akapit) Prognozowana częstość dotrenowywania Złożoność obliczeniowa algorytmu Ilość i złożoność danych. Rekomendowana architektura (implementacja w diagrams.net z łączami do GitHub (artefakty i komponenty)) Dokumentacja implementacji: Komponenty Artefakty Środowisko uruchomieniowe Dyskusja wyników (ekrany z poszczególnych faz): Dane Modelowanie Monitoring. "],["architektury-it.html", "Rozdział 2 Architektury IT ", " Rozdział 2 Architektury IT "],["potoki-danych.html", "2.1 Potoki danych", " 2.1 Potoki danych Na typową architekturę IT składają się producenci danych, ich konsumenci i system je przetwarzające. Kluczowe pytania, na które warto odpowiedzieć projektując architekturę IT, to: Skąd pozyskamy dane? W jaki sposób je pozyskamy? W jaki sposób będziemy je przetwarzać? Jak je będziemy gromadzić? Dokąd te dane później trafią? Najważniejsze powody, dla których warto projektować architektury IT, to: Rozwiązanie klasycznych problemów z danymi, takich jak: zmieniające się schematy baz i scenariusze użycia rosnąca ilość danych błędy w danych duplikacja danych wycieki danych opóźnienia (latencja) awarie w procesach konieczność manualnego zarządzania procesami IT. Integracja silosów informacyjnych często obecnych w firmach (osobne systemy wspomagające komunikację, zarządzanie różnymi obszarami działania, etc.) 2.1.1 Typy danych Najważniejsze typy danych, które napotkamy w projektach uczenia maszynowego, to: Dane o zdarzeniach, obiektach i ich agregaty Dane ustrukturyzowane, nieustrukturyzowane i częściowo-ustrukturyzowane. Dane o obiektach (ang. entity data) przedstawiają najczęściej stan obiektu, np. użytkownika, produktu, zamówienia. Dane o zdarzeniach (ang. event data) opisują działania wykonywane przez (lub na) obiektach. Najbardziej typowe cechy takich zdarzeń to: identyfikator typ zdarzenia znacznik czasu informacje uzupełniające. Warto podkreślić, że współczesne systemy generują zdecydowanie więcej danych o zdarzeniach niż danych o podmiotach (na każdego użytkownika korzystającego z aplikacji mogą przypadać tysiące zdarzeń). Dane o zdarzeniach mogą być agregowane np. w celu analiz biznesowych (np. KPIs). Przykładowo, często wykorzystywane w biznesie miary będące wynikiem agregacji danych o obiektach i wydarzeniach to: Liczba aktywnych użytkowników Dzienni aktywni użytkownicy (DAU) Tygodniowi aktywni użytkownicy (WAU) Miesięczna liczba aktywnych użytkowników (MAU) Długość sesji Czas spędzony przez użytkownika korzystającego z Twojej aplikacji podczas jednej sesji. Współczynnik kliknięć (CTR) Stosunek liczby użytkowników, którzy kliknęli na reklamę lub banner do liczby użytkowników, którzy obejrzeli stronę z tą reklamą lub bannerem. Współczynnik odrzuceń (BR) Procent użytkowników opuszczających witrynę po obejrzeniu tylko jednej strony. Współczynnik konwersji (CR) Procent użytkowników, którzy wykonują pożądaną akcję. Relację pomiędzy danymi o zdarzeniach i obiektach można podsumować następująco: Kolejną istotną w uczeniu maszynowym charakterystyką danych jest ich podział na dane ustrukturyzowane, częściowo-ustrukturyzowane i nieustrukturyzowane. Dane ustrukturyzowane: Są uporządkowane w tabelach Można określić między nimi relacje Można odpytywać korzystając z języka SQL (Structured Query Language) Wymagają wskazania schematu (Schema): sposobu organizacji danych. Dane częściowo-ustrukturyzowane: Nie są zgodne z relacyjnymi bazami danych, takimi jak Excel czy SQL, ale mają pewien poziom organizacji, np. znaczniki. Nie są ściśle relacyjne Po przetworzeniu mogą być przechowywane w: relacyjnych bazach danych bazach NoSQL plikach CSV, XML i JSON. Dane nieustrukturyzowane: Najczęściej dane jakościowe Nie posiadają schematu/modelu, czy też relacji Można je składować w bazach NoSQL i jeziorach danych Przykłady: pliki audio, video, dokumenty tekstowe, wpisy na forach dyskusyjnych, etc. 2.1.2 Bazy danych Technologie gromadzenia danych powinny być dostosowane do ich typu. 2.1.2.1 Bazy SQL Dane ustrukturyzowane Uporządkowane w tabelach Można określić między nimi relacje Można odpytywać językiem SQL (Structured Query Language) Wymagają wskazania schematu (Schema): sposobu organizacji danych W efekcie, gromadzimy najczęściej w bazach SQL. Kluczową technologią w tego typu systemach jest OLTP (OnLine Transaction Processing). Przykładowe bazy SQL: Oracle, MS SQL Server, MySQL, PostgreSQL Przykładowe zastosowania: systemy finansowe, transakcyjne, ERP, etc. 2.1.2.2 Bazy NoSQL Bazy NoSQL służą do przechowywania danych nieustrukturyzowanych. Wyróżniamy 4 podstawowe typy baz NoSQL. Bazy zorientowane na dokumenty Bazy kolumnowe Bazy oparte o wartości kluczy (key-value) Bazy grafowe. Bazy zorientowane na dokumenty: Nie posiadają ustalonego schematu Dane składowane w dokumentach JSON (JavaScript Object Notation) Każdy dokument może mieć inny zestaw pól Przykładowe bazy: MongoDB, CouchDB, DocumentDB Przykładowe zastosowania: systemy zarządzania dokumentami. Bazy kolumnowe Dane składowane są w nich w kolumnach (nie w wierszach) W efekcie, operacje (zapytania, dodawanie, kasowanie, etc.) oparte na kolumnach działają w nich bardzo szybko Przykładowe bazy: Cassandra Przykładowe zastosowania: zaawansowane analizy danych. Bazy oparte o wartości kluczy (key-value) Każdy wpis ma w nich unikatowy klucz Efekt: umożliwiają szybszy zapis i odczyt danych Przykładowe bazy: Redis, Amazon Dynamo DB Przykładowe zastosowania: opinie klientów. Bazy grafowe Dane skladowane w formie sieci Koncentracja na połączeniach (relacjach) pomiędzy punktami (obiektami) Wykorzystywane w analizach relacji Przykładowe bazy: Neo4j, Inifinite Graph Przykładowe zastosowania: analiza sieci społecznych. 2.1.3 Hurtownie danych Bazy danych, oparte na technologii OLTP (OnLine Transaction Processing) są zaprojektowane w celu zapewnienia efektywnego działania systemów transakcyjnych. Ich celem nie jest optymalizacja analityki Hurtownie danych: oparte są o technologie OLAP (Online Analytical Processing), wspomagające użytkowników w interaktywnej analizie wielowymiarowych danych, w szczególności: Konsolidacji (grupowania) Drążenia (drill-down) Przekrojów danych Hurtownie danych wykorzystują dane zgromadzone w bazach danych (OLTP), tworząc warstwę zoptymalizowaną pod kątem zastosowań analitycznych. W efekcie, hurtownie danych integrują dane z różnych źródeł, będąc często centralnym repozytorium informacji zoptymalizowane pod kątem analityki biznesowej. Źródłami danych dla hurtowni danych są różne systemy transakcyjne i inne bazy danych. Główne zalety i korzyści ze stosowania hurtowni danych to: Konsolidacja danych w jednym miejscu Szybsze analizy biznesowe Ułatwione procesy transformacji i wzbogacania danych oraz inżynierii cech Poprawa jakość danych. Wady i ograniczenia hurtowni danych: Mogą być kosztowne Wymagają ciągłej opieki (czyszczenie, transformacja, integracja danych, …) Bywają zbyt złożone w przypadku doraźnych potrzeb analitycznych. Hurtownie danych warto stosować w danej organizacji: Jest wiele rozproszonych baz danych/systemów dziedzinowych Jest wiele różnych baz danych Jest gromadzona duża ilość danych historycznych. "],["jeziora-danych.html", "2.2 Jeziora danych", " 2.2 Jeziora danych Jeziora danych to repozytoria, które przechowują dane w ich naturalnej postaci. Stanowią zazwyczaj pojedynczy zbiór wszystkich danych przedsiębiorstwa. Są źródłem danych dla systemów umożliwiające raportowanie, wizualizację, zaawansowaną analitykę i uczenie maszynowe. Główne zalety jezior danych: Można w nich przechowywać duże ilości danych… …które mogą mieć różne formy: Ustrukturyzowane Cześciowo ustruktyryzowane Nieustrukturyzowane Przetwarzanie przed załadowaniem nie jest wymagane. Główne wady i ograniczenia jezior danych to: To technologia, która wciąż się rozwija Problemy ze specjalistami Zarządzanie danymi może być uciążliwe Niskie koszty mogą stymulować gromadzenie danych niepotrzebnych Prywatność danych: dane całej organizacji w jednym repozytorium. Jeziora danych warto stosować w następujących sytuacjach: Eksperymenty Data Science: chcemy sprawdzić proof-of-concept architektury przed zainwestowaniem w profesjonalny potok danych Do analizy danych w obszare cyberbezpieczeństwo: gromadzenie logów z wielu urządzeń w celu analizy pod kątem bezpieczeństwa Do analizy danych o klientach: gromadzenie i analiza danych o zachowaniach klientów z wielu źródeł i kanałów (www, mobile, sklepy tradycyjne, e-commerce, systemy lojalnościowe, CRM, etc.). "],["konsumenci-danych.html", "2.3 Konsumenci danych", " 2.3 Konsumenci danych Analizę potrzeb, która będzie podstawą dla projektu architektury systemu IT/uczenia maszynowego, warto rozpocząć od zbadania potrzeb użytkowników końcowych (konsumentów danych), podstawowych celów biznesowych i typowych scenariuszy użycia. Kluczowe pytania, które warto zadać na tym etapie, to: W jaki sposób konsument danych chce z nich korzystać? Do raportowania, tworzenia wizualizacji, podejmowania decyzji, a może do budowania modeli Machine Learning (ML)? Jakie narzędzia są aktualnie używane przez użytkowników? Microsoft Excel, Tableau, Microsoft Power BI lub Google Data Studio? Czy istnieją jakieś standardy w ramach danej grupy użytkowników? Dział prawny może potrzebować danych w innej postaci niż księgowość czy finanse. Możliwe cele biznesowe to: Najbardziej typowe scenariusze użycia: "],["producenci-danych.html", "2.4 Producenci danych", " 2.4 Producenci danych Typowe źródła danych o zdarzeniach to: Strony www generujące dane o zachowaniach użytkownika: Pobrania Kliknięcia Wypełnienie formularza Komentarze Media społecznościowe: Publikacja wpisu Udostępnienie obiektu (wpis, zdjecie, film, …) Polubienie obiektu Hashtag Wystawienie opinii Systemy IT, generujące sygnały takie jak: Replikacja danych Synchronizacja danych Uruchomienie zadania Wykasowanie zadania, etc. Sensory, np. Detektory ruchu Detektory głosu Detektory temperatury Detektory dymu, etc. Typowe źródła danych o obiektach to: Systemy transakcyjne i dziedzinowe (ERP, CRM, etc.) Bazy danych Hurtownie danych Pliki i (rozproszone) systemy plików Źródła zewnętrzne, API (Application Programming Interface). "],["transformacja-danych.html", "2.5 Transformacja danych", " 2.5 Transformacja danych Dane pozyskane ze źródeł są najczęściej przetwarzana na dwa różne sposoby: ETL: Extract &gt; Transform &gt; Load ELT: Extract &gt; Load &gt; Transform. 2.5.1 Przetwarzanie ETL (Extract, Transform, Load) Ekstrakcja danych (ang. extract) to czynność lub proces pobierania danych ze źródeł danych w celu ich dalszego przetwarzania lub przechowywania. Transformacja danych (ang. Transform) to zbiór reguł lub funkcji stosowanych do pozyskanych danych w celu przygotowania ich do załadowania do docelowego systemu. Ładowanie danych (ang. load) polega na przekazaniu danych do docelowego magazynu: płaskiego pliku, bazy czy hurtowni. Dane mogą być przetwarzane wsadowo lub w sposób ciągły (strumieniowe). Przetwarzanie wsadowe (ang. batch processing) polega na jednoczesnym przetwarzaniu dużej ilości danych. Przetwarzanie strumieniowe (stream processing) odbywa się w czasie zbliżonym do rzeczywistego - dane są przetwarzane w miarę ich napływu. Przykłady: przetwarzanie płatności i wykrywanie oszustw. Wyzwania związane ze stosowaniem przetwarzania ETL: Zbyt dużo danych. Ilość danych generowanych rośnie Programy służące do transformacji mogą liczyć miliony linii. Może to bardzo utrudnić skalowanie potoku ETL. Przekształcanie wszystkich danych przed ich załadowaniem może być zbyteczne. Przykładowo może się okazać, że nie ma potrzeby przetwarzania wszystkich danych o zdarzeniach generowanych na stronie internetowej jednocześnie. Różne typy danych Różne typy danych (ustruktyrozowane, nie ustruktyryzowane, o obiektach czy zdarzeniach) wymagają różnych metod transformacji. ETL najlepiej sprawdza się w przypadku ustrukturyzowanych danych o obiektach (structured, entity). W efekcie, ETL warto stosować, w sytuacji, gdy dysponujemy dużą ilości ustrukturyzowanych danych transakcyjnych. 2.5.2 Przetwarzanie ELT (Extract, Load, Transform) W przetwarzaniu ELT: dane różnego typu (ustrukturyzowane, nieustrukturyzowane lub częściowo ustrukturyzowane) pobierane są z różnych źródeł danych a następnie ładowane do magazynu danych, np. jeziora danych. Transformacja następuje po załadowaniu do jeziora, po czym przetransformowane dane przekazywane są do dalszego wykorzystania przez ich konsumentów. Wyzwania przetwarzania ELT: Są drogie: Dużo danych różnego typu Wymagają skalowalności Wymagają dużych zasobów (składowanie, przetwarzanie) Odpowiednie technologie są stosunkowo nowe: i w efekcie mogą być mniej niezawodne niż ETL trudno w związku z tym o specjalistów i trudniej zapewnić bezpieczeństwo. ELT warto stosować: Gdy gromadzimy duże ilości danych Nie ma możliwości przetwarzać ich przed załadowaniem Dane są nieustrukturyzowane lub mieszane. Przykładowe zastosowania: dane do analizy sentymentu (opinie, e-mail’e, gwiazdki), dane z logów systemowych, etc. "],["strategia-wdrożenia.html", "2.6 Strategia wdrożenia", " 2.6 Strategia wdrożenia 2.6.1 Własne czy gotowe? Jedną z pierwszych decyzji, którą należy podjąć już podczas projektowania architektury IT, jest to, czy wykorzystamy rozwiązanie gotowe, czy też stworzymy własne? Generalnie, rekomenduję kierować się następującą zasadą: w pierwszej kolejności sprawdź, czy dane rozwiązanie jest dostępne na rynku? Jeśli tak: wykorzystaj je. dalej, sprawdź, czy można dostosować jakieś rozwiązanie do Twoich potrzeb … a dopiero jeśli nie jest możliwe skorzystanie z rozwiązania gotowego, nie ma też niczego, co można by dostosować: stwórz własne rozwiązanie. Kup gotowe, gdy…: całkowity koszty zakupu jest dużo niższy niż wytworzenie budowa zajmie zbyt dużo czasu dla w miarę uniwersalnych problemów biznesowych (np. HR) jeśli Twoje problemy biznesowe już zostały przez kogoś rozwiązane, i na rynku są już liderzy takich rozwiązań. Kupno gotowego rozwiązania przyspiesza zarówno wejście na rynek, jak i jego skalowanie w przyszłości Stwórz własne, gdy…: obszar obsługiwany systemem jest kluczowym czynnikiem przewagi konkurencyjnej koszty dostosowania gotowego produktu są bardzo duże, całkowity koszty wytworzenia jest dużo niższy (programowanie, testowanie, konfiguracja, skalowanie, ludzie (specjaliści), infrastruktura) wytworzenie wymaga głębokiego zrozumienia specyfiki biznesu bardzo zależy Ci na czasie, a dostawcy nie mogą zagwarantować realizacji niezbędnego zakresu w terminie. 2.6.2 Otwarte czy komercyjne? Decyzja o tym, czy zakupić rozwiązanie licencjonowane, czy też budować własne w oparciu o technologie otwarte (ang. open-source), to kolejny dylemat, przed którym stoją kierownicy projektów czy projektanci architektur systemów IT. Wybierz rozwiązanie otwarte (open-source), gdy: ktoś już stworzył to, czego potrzebujesz? kod jest dobrze wspierany przez dużą i aktywną społeczność (np. na dużo twórców i commit’ów na GitHub.com) całkowity koszt posiadania takiego rozwiązania (TCO: Total Cost of Ownership) jest niski koresponduje to z wizerunkiem Twojej marki potrzebujesz nie tylko aplikacji, ale też kodu źródłowego. Wybierz rozwiązanie komercyjne, gdy: jest prostsze w dostosowaniu do Twoich potrzeb i wdrożeniu jest standardem branżowym (np. arkusze kalkulacyjne) potrzebujesz wiarygodnego wsparcia całkowity koszt posiadania rozwiązania otwartego (TCO: Total Cost of Ownership) jest wysoki gdy musisz zagwarantować, że nie naruszasz czyichś praw autorskich istotne jest bezpieczeństwo (choć nie zawsze…). 2.6.3 U siebie czy “w chmurze”? Systemy IT/uczenia maszynowego możesz utrzymywać na własnej infrastrukturze, lokalnie w organizacji, lub też “w chmurze” (prywatnej lub publicznej). Utrzymuj na swoich serwerach, gdy: chcesz mieć pełną kontrolę nad infrastrukturą i danymi całkowite koszty posiadania będą niższe niż w chmurze będziesz musiał często przesyłać duże ilości danych z własnych rozwiązań do “chmury” (egress) i z powrotem (ingres) rozwiązanie chmurowe może generować nieakceptowalne problemy z latencją (szybkością odpowiedzi serwera) stabilność działania jest krytyczna, i niemożliwa do zapewnienia przez dostawcę. Utrzymuj w chmurze, gdy: zależy Ci na koncentracji na swoim biznesie (uwolnieniu uwagi z konieczności monitoringu i rozwoju IT) chcesz obniżysz koszty posiadania i utrzymania infrastruktury IT istotna jest elastyczność skalowania rozwiązania (sezonowość, rozwój) potrzebujesz ciągłego, wiarygodnego wsparcia zasady firmy i reguły bezpieczeństwa dopuszczają takie rozwiązanie. "],["cykl-życia-projektu-uczenia-maszynowego-i-modelowanie-architektur.html", "Rozdział 3 Cykl życia projektu Uczenia Maszynowego i modelowanie architektur", " Rozdział 3 Cykl życia projektu Uczenia Maszynowego i modelowanie architektur Na typowy projekt uczenia maszynowego składają się następujące etapy: Przygotowanie danych Modelowanie (trening i ewaluacja) Udostępnianie modelu Monitoring Dotrenowywanie. Za realizację każdego z nich odpowiada najczęściej osobny komponent (na rysunku powyżej zaprezentowany w formie owalnej), który przyjmuje na wejściu oraz generuje na wyjściu tzw. artefakt (oznaczony jako prostokąt). W realnych projektach sytuacja jest bardziej złożona: na przygotowanie danych, modelowanie, udostępnianie i monitoring składa się wiele etapów cząstkowych (realizowanych przez odpowiednie komponenty): Przygotowanie danych pozyskanie danych walidacja danych transformacja danych Modelowanie: trening i ewaluacja walidacja finalnego modelu tuning hiperparametrów Udostępnianie modelu przygotowanie do udostępnienia (ciągła) integracja (CI: Continuos Integration) (ciągłe) uruchamianie (CD: Continuos Deployment) Monitoring monitoring i identyfikacja dryfu danych monitoring i identyfikacja dryfu modeli monitoring infrastruktury Dotrenowywanie generowanie sygnału dotrenowania wybór nowych danych treningowych dotrenowanie. Realizację tych procesów wspiera wiele dedykowanych systemów, wyspecjalizowanych w realizacji konkretnych zadań. Szczególnie istotną rolę odgrywają: repozytoria cech (ang. feature stores) systemy monitoringu eksperymentów repozytoria modeli (ang. model stores) systemy ciągłej integracji i udostępniania (ang. continuous integration (CI) and deployment (CD)) systemy monitorujące modele repozytoria metadanych generowanych przez cały proces (ang. metadata stores). Wybór technologii wykorzystanych do implementacji poszczególnych komponentów zależy od stopnia złożoności projektu. W prostych przedsięwzięciach można z powodzeniem wykorzystać pliki CSV, skrypty Python czy aplikacje typu Flask, bardziej złożone wymagają wdrożenia dedykowanych rozwiązań dostępnych za darmo (np. MLFlow, FastApi czy darmowa wersja WandB.ai), zaś zaawansowane dedykowanych systemów takich jak Feast, Kubeflow, Heroku czy Arize. Pejzaż dostępnych w tym zakresie rozwiązań jest bardzo dynamiczny: pojawia się tu coraz więcej nowych rozwiązań, czemu towarzyszy mniej lub bardziej dynamiczny rozwój już istniejących. Przydatne źródła Przykład kompletnej architektury projektu uczenia maszynowego, ze świetnie opisanymi poszczególnymi etapami jej konstrukcji, opisany jest tutaj. Bardzo dobrym wprowadzenie w zagadnienie cyklu życia projektu maszynowego, i ogólniej w tematykę MLOps jest artykuł dostępny tutaj. Nieco bardziej rozbudowane wprowadzenie do różnych poziomów automatyzacji procesów MLOps można znaleźć tutaj: poziom 0: manual pipelines poziom 1: continuous training poziom 2: CI/CD Interesujący przegląd narzędzi wspomagających zarządzanie cyklem życia oraz poszczególnymi etapami projektu data science jest dostępny tutaj. "],["demo-trening-i-ewaluacja-modelu.html", "3.1 Demo: trening i ewaluacja modelu", " 3.1 Demo: trening i ewaluacja modelu Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 3.1.1 Cel Nasz pierwszy kod będzie stanowił punkt wyjścia dla kolejnych ćwiczeń. Stworzymy sekwencję skryptów Python, których zadaniem będzie wczytanie danych treningowych, stworzenie modelu i jego ewaluacja na danych testowych. Do modelowania wykorzystamy dane syntetyczne, które będą możliwe do zamodelowania z wykorzystaniem prostego modelu regresji liniowej. Stworzony dzięki temu szkielet oprogramowania będziemy mogli później rozwijać w następujących wymiarach: Urealnienie problemu: przejście od danych syntetycznych do danych realnych, np. modelowania cen mieszkań zmianę typu problemu uczenia maszynowego: przejście od regresji do np. klasyfikacji Zamiany prostych skryptów Python na dedykowane biblioteki, służące np. do monitorowania modeli czy pre-processingu danych Zmiany środowisk uruchomieniowych: z własnego komputera na docker czy kubernetes. W ćwiczeniu tym przedstawimy też podstawowe zasady projektowania architektury kodu uczenia maszynowego, w szczególności pojęcia artefaktów, komponentów i wizualizacji relacji pomiędzy nimi z wykorzystaniem diagramów. 3.1.2 Lista kontrolna Skrypt, który stworzymy, będzie realizował następujące zadania: Wczytanie danych ​ Wczytuje plik treningowy data_init.csv ​ Zapisuje go do pliku data_train.csv Trenowanie modelu ​ Wczytuje dane treningowe data_train.csv ​ Przygotowuje dane do modelowania ​ Trenuje model korzystając algorytmu LinearRegression ​ Drukuje na ekranie parametry modelu ​ Zapisuje model do pliku model_1.0.pkl w folderze model Ewaluacja modelu ​ Wczytuje model z pliku model/model_1.0.pkl ​ Wczytuje dane treningowe z pliku data/data_test.csv ​ Generuje predykcje i ocenia model ​ Drukuje wyniki ewaluacji na ekranie komputera 3.1.3 Architektura 3.1.3.1 Artefakty Wejście Inicjalny plik treningowy: data_init.csv Plik testowy: data_test.csv Wyjście model: model_1.0 3.1.3.2 Komponenty Wczytanie danych (1. read.py) Wejście: data_init.csv Działania: wczytuje dane zapisuje na dysku Wyjście: data_train.csv Trening (2. train.py): Wejście: data_train.csv Działania: Wczytuje dane treningowe data_train.csv Przygotowuje dane do modelowania Trenuje model korzystając algorytmu LinearRegression Drukuje na ekranie parametry modelu Zapisuje model do pliku model_1.0.pkl w folderze model Wyjście: model_1.0.pkl Ewaluacja (3. test.py): Wejście: data_test.csv model_1.0.pkl Działania: Wczytuje model z pliku model/model_1.0.pkl Wczytuje dane treningowe z pliku data/data_test.csv Generuje predykcje i ocenia model Drukuje wyniki ewaluacji na ekranie komputera Wyjście: ekran 3.1.4 Decyzje Projektując to rozwiązanie, musimy podjąć następujące decyzje: Wybór miary jakości modelu RMSE R2 Algorytm(y) ML: LinearRegression Podsumowanie "],["modelowanie-architektury.html", "3.2 Modelowanie architektury", " 3.2 Modelowanie architektury Początkowo proste przepływy zadań w projektach uczenia maszynowego mogą z czasem ewoluować w bardzo złożone. Dlatego od samego początku warto stosować metody projektowania zapewniające kontrolę nad architekturą całości. Rozwiązania takie jak Kubeflow czy Dataiku pozwalają wizualizować zaimplementowane już przepływy, jest to jednak tylko wizualizacja przepływu zaimplementowanego wcześniej np. w formie plików JSON czy skryptów Python. Do zaprojektowania przepływu uczenia maszynowego można wykorzystać systemy do wpomagające modelowanie procesów. Przykładowo, dostępne na licencji otwartej Diagrams.net, oferowane w szczególności jako aplikacja Google. Bloki diagrams.net możemy wykorzystać do wizualizacji komponentów i artefaktów przepływu ML. Każdy blok może mieć dedykowany zestaw cech (properties), które można wykorzystać do wprowadzenia własnych meta-opisów. Szczególną cechą jest link do zasobu zewnętrznego skojarzonego z blokiem. Jeśli wykorzystamy go do połączenia z repozytorium GitHub, rozwiązanie będzie nam służyć nie tylko do wizualizacji architektury, ale też monitoringu aktualnego stanu komponentów i artefaktów. Przykładowo, każdy współpracownik w projekcie będzie miał dostęp do aktualnych miar jakości modelu. W efekcie, proste, darmowe systemy wspomagające modelowanie procesów czy architektur mogą posłużyć do stworzenia prostych systemów zarządzania procesami MLOps. 3.2.1 Proces modelowania Modelowanie architektur systemów uczenia maszynowego można zrealizować w następujących krokach: Organizacja Specyfikacja wymagań Modelowanie. 3.2.1.1 Organizacja Cały proces warto rozpocząć od stworzenia środowiska pracy (zarówno własnej, jak i grupowej). W pierwszej kolejności załóż dedykowany folder (opcjonalnie: udostępnij go współpracownikom), skonfiguruj środowisko zarządzania projektem i utwórz dedykowane repozytorium w systemi Git (np. GitHub). Następnie zdefiniuj problem i określ zakres modelowania. Chcesz modelować architekturę tylko dla procesu trenowania? A może wnioskowania na produkcji? Czy też zaprojektować pełny cykl MLOps. Przydatny może być tu artykuł dostępny tutaj. W ostatnim kroku stwórz w dedykowanym folderze pliki odpowiadające pierwszej wersji architektury (w oparciu o swoją intuicję i poprzednie doświadczenia, na razie bez tworzenia szczegółowej specyfikacji). Mogą być to szablony (puste pliki lub zawierające jedynie podstawowe elementy) komponentów, artefaktów oraz ich dokumentacji. Na koniec wprowadź tę strukturę do repozytorium GitHub. Przydatne źródła Świetny opis procesu projektowania pełnej architektury MLOps znajdziesz tutaj. 3.2.1.2 Specyfikacja wymagań Mając już przygotowane środowisko pracy, przystąp do tworzenia specyfikacji wymagań. Sugeruję w tym celu określić: wymagania funkcjonalne w formie listy kontrolnej specyfikację artefaktów specyfikację komponentów kluczowe decyzje, które należy podjąć implementując rozwiązanie. Lista kontrolna: Specyfikuje zakres działania komponentu Określa standard wykonania Jest zbiorem kryteriów odbioru Artefakty to lista obiektów (najczęściej plików z danymi lub modeli), które mogą być: Wsadem do komponentu („wejście”) Powstawać w wyniku uruchomienia komponentu („wyjście”). W przypadku plików warto określić: Miejsca i nazwy Formatu W przypadku danych przydatne jest też wskazanie: Nazw i znaczenia poszczególnych pól Typów zmiennych (np. numeryczne, kategorialne, czas, etc.). Specyfikacja komponentów określa: Wejścia (artefakt wejściowy) Procedury działań (w oparciu o listę kontrolną) Wyjścia (artefakt wyjściowy). Działania realizowane przez komponent warto pisać tak, by później posłużyły jako dokumentacja wewnętrzna kodu (ang. one-line docstrings, por. np. https://peps.python.org/pep-0257/). Czynność tę i tak z pewnością trzeba będzie wykonać w przyszłości. Realizacja tego zadania PRZED programowaniem: Ułatwi zrozumienie celu i idei działania kodu Może istotnie uprościć samo programowanie w środowiskach IDE wykorzystując technologie typu GitHub Copilot (https://copilot.github.com). W kolejnym kroku wskaż najważniejsze decyzje, jakie trzeba będzie podjąć tworzyć dany komponent. Mogą być one później podjęte np. podczas spotkań zespołu projektowego. Całość podsumuj np. w prostym arkuszu i zapisz w folderze ze specyfikacją projektu. 3.2.1.3 Modelowanie Mając przygotowaną szczegółową specyfikację działania modułu, stwórz szkic architektury korzystając np. z pakietu diagrams.net (możesz z niego korzystać w ramach aplikacji Google). Umieść na schemacie: komponenty artefakty oraz relacje pomiędzy nimi. W kolejnym kroku, podłącz poszczególne komponenty i artefakty do odpowiednich plików w repozytorium GitHub. Aby podłączyć specyfikację artefaktu/komponentu stwórz dedykowane pole (np. Description), i wklej do niego hiperłącze do specyfikacji. Uwaga: ze względu na ograniczoną liczbę znaków, sugeruję wcześniej skrócić link do dokumentacji korzystając z serwisu takiego jak www.bit.ly. Z czasem rozwój systemu wymusza modyfikację architektury. Aby utrzymać przejrzystość architektury, wart porządkować komponenty i artefakty w osobne struktury. Przydatne może być wtedy uporządkowanie poszczególnych etapów modelowania w osobnych folderach. W pakiecie diagrams.net mogą nam w tym pomóc pionowe kontenery (ang. vertical containers). Pozwalają one na grupowanie komponentów i artefaktów, i w efekcie uproszczenie całej wizualizacji. 3.2.1.4 Podsumowanie Tworzenie aplikacji warto rozpocząć od jej zaprojektowania. Projektowanie architektur uczenia maszynowego nie musi być poprzedzone ich tworzeniem. Z powodzeniem można wykorzystać w tym celu ogólnodostępne, darmowe rozwiązania wspomagające projektowanie schematów. Korzyści z takiego podejścia to: Wizualizacja całości rozwiązania (komponenty, artefakty i relacje pomiędzy nimi) Szybki dostęp do aktualnych wersji komponentów i artefaktów dzięki połączeniu z GitHub Tworzenie własnych meta-opisów dzięki personalizowanym cechom obiektów. Przydatne źródła Świetny opis procesu projektowania pełnej architektury MLOps znajdziesz tutaj. "],["demo-modelowanie.html", "Rozdział 4 Demo: modelowanie", " Rozdział 4 Demo: modelowanie Na tym krótkim filmie pokazuję, w jaki sposób można wykorzystać oprogramowanie Diagrams.net do stworzenia prostej architektury systemu uczenia maszynowego. Poniżej znajdziesz listę kontrolną, specyfikacje artefaktów i komponentów oraz decyzje, jakie należało podjąć w procesie projektowania. "],["lista-kontrolna-1.html", "4.1 Lista kontrolna", " 4.1 Lista kontrolna Skrypt, który stworzymy, będzie realizował następujące zadania: Wczytanie danych ​ Wczytuje plik treningowy data_init.csv ​ Zapisuje go do pliku data_train.csv Trenowanie modelu ​ Wczytuje dane treningowe data_train.csv ​ Przygotowuje dane do modelowania ​ Trenuje model korzystając algorytmu LinearRegression ​ Drukuje na ekranie parametry modelu ​ Zapisuje model do pliku model_1.0.pkl w folderze model Ewaluacja modelu ​ Wczytuje model z pliku model/model_1.0.pkl ​ Wczytuje dane treningowe z pliku data/data_test.csv ​ Generuje predykcje i ocenia model ​ Drukuje wyniki ewaluacji na ekranie komputera "],["architektura-1.html", "4.2 Architektura", " 4.2 Architektura 4.2.1 Artefakty Wejście Inicjalny plik treningowy: data_init.csv Plik testowy: data_test.csv Wyjście model: model_1.0 4.2.2 Komponenty Wczytanie danych (1. read.py) Wejście: data_init.csv Działania: wczytuje dane zapisuje na dysku Wyjście: data_train.csv Trening (2. train.py): Wejście: data_train.csv Działania: Wczytuje dane treningowe data_train.csv Przygotowuje dane do modelowania Trenuje model korzystając algorytmu LinearRegression Drukuje na ekranie parametry modelu Zapisuje model do pliku model_1.0.pkl w folderze model Wyjście: model_1.0.pkl Ewaluacja (3. test.py): Wejście: data_test.csv model_1.0.pkl Działania: Wczytuje model z pliku model/model_1.0.pkl Wczytuje dane treningowe z pliku data/data_test.csv Generuje predykcje i ocenia model Drukuje wyniki ewaluacji na ekranie komputera Wyjście: ekran "],["decyzje-1.html", "4.3 Decyzje", " 4.3 Decyzje Projektując to rozwiązanie, musimy podjąć następujące decyzje: Wybór miary jakości modelu RMSE R2 Algorytm(y) ML: LinearRegression "],["podsumowanie-1.html", "4.4 Podsumowanie", " 4.4 Podsumowanie "],["podsumowanie-2.html", "Rozdział 5 Podsumowanie", " Rozdział 5 Podsumowanie Cykl życia modelu uczenia maszynowego jest złożony - dlatego już na samym początku projektu zacząć pracować nad architekturą docelowego rozwiązania. Większość rozwiązań wizualizuje architekturę w oparciu o już stworzony kod. Do zaprojektowania przepływu uczenia maszynowego można jednak wykorzystać systemy do wpomagające modelowanie procesów. Przykładowo, dostępne na licencji otwartej Diagrams.net, oferowane w szczególności jako aplikacja Google. Proces modelowania proponujemy zrealizować w trzech krokach: rozpocząć od organizacji, potem stworzyć wstępną specyfikację, a następnie model architektury. Jeśli wykorzystamy go do połączenia z repozytorium GitHub, rozwiązanie będzie nam służyć nie tylko do wizualizacji architektury, ale też monitoringu aktualnego stanu komponentów i artefaktów. Przykładowo, każdy współpracownik w projekcie będzie miał dostęp do aktualnych miar jakości modelu. "],["monitoring-modeli.html", "Rozdział 6 Monitoring modeli", " Rozdział 6 Monitoring modeli Po udostępnieniu model uczenia maszynowego jest wykorzystywany w środowisku produkcyjnym. Jakość jego predykcji (lub innych zadań, do których został wytrenowany) może być oceniana poprzez porównanie zgodności prognoz z rzeczywistością. Przykładowo, model prognozujący popyt na wybrany produkt w danym horyzoncie czasowym (np. tygodnia) może być oceniony po upływie tego czasu poprzez porównanie predykcji z faktycznym popytem. Etap ten nazywamy ewaluacją. Artefakty procesu ewaluacji to najczęściej miary efektów (ang. scoring) oraz zbiory doświadczeń (zawierające najczęściej stempel czasu, prognozę oraz stan faktyczne). image-20220430121920747 Modele uczenia maszynowego nie są doskonałe. Ich jakość zależy od typu problemu (klasyfikacja? regresja? predykcja szeregu czasowego? segmentacja?) i jest mierzona w procesie trenowania (przed akceptacją do wdrożenia produkcyjnego). Często okazuje się, że początkowo bardzo dobra jakość modelu, z czasem istotnie się obniża. Innymi słowy, model początkowo świetnie prognozujący np. popyt na dany produkt po pewnym czasie zaczyna popełniać coraz więcej błędów. Przyczyn takiej sytuacji jest kilka. Po pierwsze, może wystąpić zmiana w zmiennej prognozowanej. Przykładowo, w przypadku sprzedaży inflacja może w naturalny sposób podnieść poziom wszystkich cen, czego efektem może być podniesie wolumenu sprzedaży - model wytrenowany na danych historycznych będzie w takiej sytuacji prognozował niższe wartości niż realne. Po drugie, mogą zmienić się zmienne objaśniające (cechy). Użytkownicy mogą zmieniać urządzenia, z których korzystają, na rynku mogą pojawiać się produkty nowego typu, etc. Przykładowo, model prognozujący popyt na mieszkania wytrenowany na danych, w których były dostępne jedynie budynki 3-4 piętrowe przestaną działać w sytuacji, gdy w mieście pojawią się 10-piętrowe wieżowce. Kolejną przyczyną deaktualizacji modeli mogą być zmiany w relacjach pomiędzy cechami a zmiennymi prognozowanymi. W naturalny sposób mogą zmieniać się gusta klientów, pojawiać nowe trendy zakupowe, nowe kampanie marketingowe, konkurencja, substytuty produktów czy też po prostu zmiany globalne. Wszystkie te czynniki wpływają na stopniowe pogarszanie jakości modeli stosowanych produkcyjnie, co jest motywacją dla wdrożenia procesów monintoringu i udoskonalania modeli. "],["przydatne-źródła.html", "Rozdział 7 Przydatne źródła", " Rozdział 7 Przydatne źródła Podstawowe wprowadzenie do przyczyn, skutków i metod zapobiegania dryfowi modeli można znaleźć tutaj. Bardzo dobre wprowadzenie w różne strategie postępowania w przypadku detekcji dryfu modelu znajduje się tutaj. Interesujący przegląd narzędzi wspomagających monitoring modeli (uwaga: stworzony przez jednego z dostawców tego typu narzędzi) dostępny jest tutaj. "],["monitoring-modelu.html", "Rozdział 8 Monitoring modelu", " Rozdział 8 Monitoring modelu Ocena jakości modelu jest kluczowym elementem procesu trenowania: decyzję o przekazaniu modelu do wdrożenia produkcyjnego podejmujemy kierując się w dużej mierze wartością odpowiedniej miary tej jakości. "],["metody-ewaluacji.html", "8.1 Metody ewaluacji", " 8.1 Metody ewaluacji image-20220501095617406 image-20220501095630651 image-20220501095644328 Sposób oceny jakości zależy przede wszystkim od typu problemu, jaki ma rozwiązać model. Miary ewaluacji będą różne dla problemów klasyfikacji, regresji, prognozy szeregu czasowego czy segmentacji. "],["doświadczenia.html", "8.2 Doświadczenia", " 8.2 Doświadczenia image-20220501100220736 Podczas korzystania z modelu w środowisku predykcyjnym, możemy z czasem gromadzić doświadczenia: porównywać prognozę modelu z tym, co wydarzyło się faktycznie, i gromadzić te porównania w osobnym repozytorium. Doświadczenia gromadzimy w miarę upływu czasu, i dopiero po pewnym czasie możemy ocenić, na ile dobrze nasz model sprawdza się w nowej (innej niż podczas treningu) rzeczywistości. W tym celu wykorzystujemy właśnie repozytoria doświadczeń. Predykcję realizować możemy realizować: w ciągły sposób (przetwarzanie strumieniowe, predykcja on-line) … lub w pakietach (przetwarzanie pakietowe, predykcja off-line)) W efekcie, dane na potrzeby ewaluacji możemy więc pozyskiwać: rekord po rekordzie &lt; predykcja on-line w pakietach &lt; predykcja off-line. "],["potok-wnioskowania.html", "8.3 Potok wnioskowania", " 8.3 Potok wnioskowania Sekwencję działań, których celem jest monitoring jakości modelu, można uporządkować w tzw. potok wnioskowania (ang. inference pipeline). W dużym uproszczeniu, sprowadza się on do cyklicznego gromadzenia doświadczeń w dedykowanym repozytorium (bazie danych czy zwykłym pliku), obliczania miar jakości modelu i gromadzenia tych miar w kolejnym repozytorium. Na potrzeby kolejnych rozważań uprościmy notację, i pełny proces wnioskowania oznaczać będziemy symbolem jednego bloku IF. image-20220501102756426 Schemat powyżej ilustruje kolejne etapy wnioskowania produkcyjnego, których rezultatem są nie tylko predykcje, ale też kolejne doświadczenia (gromadzone w repozytorium oznaczonym symbolem T) oraz szereg czasowy miar jakości modelu (gromadzony w repozytorium oznaczonym symbolem S). Jeśli miara jakości przekroczy pewien założony przez nas próg, system może wygenerować sygnał dotrenowania. "],["podsumowanie-3.html", "8.4 Podsumowanie", " 8.4 Podsumowanie image-20220501103336176 Jak widać, cały proces uczenia maszynowego jest sekwencją wnioskowań raz na jakiś czas przerywanych sygnałem dotrenowania, który uruchamia proces udoskonalania modelu (treningu). Kluczową decyzją w tym przepływie jest określenie reguł generowania sygnału dotrenowania. "],["przydatne-źródła-1.html", "Rozdział 9 Przydatne źródła", " Rozdział 9 Przydatne źródła Interesującą platformą dedykowaną do monitoringu modeli jest https://arize.com/. Warto przeanalizować dostępne tam materiały, w szczególności https://arize.com/ml-observability/. "],["demo-monitoring-modelu.html", "Rozdział 10 Demo: monitoring modelu", " Rozdział 10 Demo: monitoring modelu Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 "],["cel-1.html", "10.1 Cel", " 10.1 Cel Celem naszego programu jest ewaluacja już wytrenowanego modelu na kolejnych partiach danych i zapis metryk jakości do osobnego pliku. W module wykorzystamy: wytrenowany w poprzednim ćwiczeniu model regresji syntetyczne zbiory danych, zbliżone do zbioru treningowego, niemniej na tyle różne, by zidentyfikować potencjalny dryf modelu. Stworzony dzięki temu program będziemy mogli później wykorzystać jako komponent procesu identyfikacji dryfu modelu. "],["lista-kontrolna-2.html", "10.2 Lista kontrolna", " 10.2 Lista kontrolna Skrypt, który stworzymy, będzie realizował następujące zadania: Wczytanie wytrenowanego modelu model/model_1.0.pkl Wczytanie danych testowych: wybór paczki danych testowych: batch_no w zakresie od 1 do 6 wczytanie odpowiedniej paczki Wygenerowanie predykcji modelu Obliczenie miar jakości: RMSE r2 Zapisanie w pliku evaluation/model_eval.csv: stempla czasowego nr paczki danych wartości miary RMSE wartości miary r2 UWAGA: jeśli pliku nie istnieje: utworzenie go w przeciwnym przypadku: uzupełnienie pliku (dodanie aktualnych rekordów). "],["architektura-2.html", "10.3 Architektura", " 10.3 Architektura 10.3.1 Artefakty Wejście Model: model/model_1.0.pkl Pliki testowe: data/batch_n.csv, z n w zakresie od 1 do 6 Wyjście Plik ewaluacyjny: evaluation/model_eval.csv 10.3.2 Komponenty Jeden program 1. Evaluate.py realizujący zadania z listy kontrolnej. "],["decyzje-2.html", "10.4 Decyzje", " 10.4 Decyzje Projektując to rozwiązanie, musimy podjąć następujące decyzje: Wybór miary jakości modelu RMSE R2 "],["ćwiczenie-monitoring-modelu.html", "Rozdział 11 Ćwiczenie: monitoring modelu", " Rozdział 11 Ćwiczenie: monitoring modelu Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 Sterowanie parametrami uruchomieniowymi z linii komendą jest powszechną, dobrą praktyką we wdrożeniach produkcyjnych. Jest nie tylko wygodne i bezpieczne (brak konieczności zmiany skryptu przy zmianie parametrów), ale zapewnia też lepszą kontrolę procesu oraz umożliwia wykorzystanie pakietów dedykowanych do skanowania hiperparametrów (takich jak https://hydra.cc/ czy https://optuna.org/) Zmodyfikuj moduł demonstracyjny tak, by nazwy artefaktów wczytywane były do pliku z linii komend, a nie wprowadzane bezpośrednio w skrypcie Python. Wykorzystaj w tym celu bibliotekę argparse. Tak zmodyfikowany plik będziesz mogła/mógł wykorzystać potem w implementacji pełnego potoku monitoringu modelu i detekcji dryfu. Lista kontrolna Zmodyfikowany skrypt umożliwia przekazanie z linii komend następujących parametrów: batch_no: nr paczki danych testowych z folderu data\\ model_path: ścieżka do pliku z wytrenowanym modelem. "],["przydatne-źródła-2.html", "Rozdział 12 Przydatne źródła", " Rozdział 12 Przydatne źródła Przystępne, ale kompleksowe wprowadzenie do biblioteki argparse znajdziesz tutaj. Hydra.cc: biblioteka umożliwiająca zaawansowane sterowanie parametrami uruchomieniowymi. Optuna.org: biblioteka oferująca inteligentne metody skanowania optymalizacji hiperparametrów w uczeniu maszynowym. "],["detekcja-dryfu-modelu.html", "Rozdział 13 Detekcja dryfu modelu", " Rozdział 13 Detekcja dryfu modelu Jak już wspominaliśmy wcześniej, dryf modelu może być spowodowany w szczególności: Zmianą zmiennej prognozowanej (np. w przypadku sprzedaży: inflacją) Zmianą zmiennych objąśniających (np. cech klientów, charakterystyk produktów, etc.) Zmianą relacji pomiędzy zmiennych prognozowanymi a cechami (np. zmiana koniunktury, wprowadzenie substytów, nowe kampanie marketingowe, etc.). Poniżej przedstawimy najczęściej spotykane metody identyfikacji dryfu modelu i generowania sygnału dotrenowania. Metoda 1. Twardy próg Najprostszą metodą identyfikacji dryfu modelu jest: ustalenie pewnego “twardego” progu: minimalnego akceptowalnego poziom jakości modelu (np. maksymalny dopuszczalny poziom błędu RMSE) uruchamienie sygnału dotrenowania w momencie, gdy miara jakości modelu przekroczy ten próg. Metoda 2. Test porównawczy Kolejna metoda, tzw. test porównawczy, uruchamia sygnał w momencie, gdy nowa miara jakości modelu jest gorsza niż wszystkie poprzednie wartości. Na rysunku miara oznaczona czerwoną kropką jest większa niż wszystkie pozostałe, co jest źródłem “alarmu”. Wadą tego rozwiązania jest często jego “nadwrażliwość”: sygnał dotrenowania uruchamiany jest zbyt często i niepotrzebnie. Metoda 3. Test istotności parametrycznej Rozwiązaniem problemu “nadwrażliwości” testu porównawczego jest test istotności parametrycznej. W tym podejściu sygnał dotrenowania uruchamiamy w sytuacji, gdy aktualna wartość miary jakości jest gorsza (mniejsza lub większa) o dwa odchylenia standardowe od średniej poprzednich wartości. Wybór mniejsza/większa zależy od tego, czy wyższa wartość miary świadczy o poprawie, czy też zmniejszeniu jakości modelu (por. rysunek powyżej). Procedura postępowania jest w tym przypadku następująca: Oblicz średnią poprzednich wartości miary Oblicz odchylenie standardowe poprzednich wartości miary Sprawdź, czy aktualna wartość miary jest gorsza (mniejsza lub większa) niż średnia +/- 2*odchylenie standardowe. Problematyczne w tym podejściu mogą być sytuacje, gdy nasze odczyty pomiarów jakości: nie układają się krzywą dzwonową mają wartości odstające. Metoda 4. Testy nieparametryczne (detekcja wartości odstających) W przypadku testów nieparametrycznych sygnał dotrenowania uruchamiamy w sytuacji, gdy aktualna wartość miary jakości jest zinterpretowana jako wartość odstająca i jest gorsza od poprzednich wartości. Procedura postępowania jest w tym podejściu następująca: Dla zbioru poprzednich miar oblicz Q1 (pierwszy kwartyl) Q3 (trzeci kwartyl) Odległość międzykwartylową: IQR = Q3 - Q1 Ustal granice wartości ”normalnych”: Dolna granica: Q1 – 1.5 IQR Górna granica: Q3 + 1.5 IQR Dla nowej miary jakości modelu sprawdź, czy mieści się ona w dopuszczalnych granicach: Q1 – 1.5 IQR &lt; miara &lt; Q3 + 1.5*IQR Wygeneruj sygnał dotrenowania w sytuacji, gdy: nowa wartość miary jest poza tymi granicami i jest gorsza niż pozostałe (mniejsza lub większa, w zależności od typu miary). Dla przypomnienia, pierwszy kwartyl to wartość miary, od której 25% wszystkich miar w zbiorze jest mniejsza. Metoda 5. Testy hipotez statystycznych Często stosowaną metodą identyfikacji dryfu modelu jest sformułowanie i weryfikacja hipotezy, że rozkład jednego zbioru danych różni się od rozkładu innego zbioru. Podejście to może służyć zarówno do identyfikacji zmiany jakości modelu, jak tzw. dryfu danych prognozowanych. Do weryfikacji takich hipotez można wykorzystać testy hipotez statystycznych. W uczeniu maszynowym szczególnie popularne są test Kolmogorowa-Smirnowa oraz test Chi-squared. Osoby zainteresowane zachęcam do lektury artykułów z sekcji Przydatne źródła. "],["przydatne-źródła-3.html", "Rozdział 14 Przydatne źródła", " Rozdział 14 Przydatne źródła Bardzo dobrą prezentację różnych metod identyfikacji dryfu modelu można znaleźć tutaj. Wprowadzenie do testowania hipotez dostępne jest w tej lekcji na Khan Academy. Wykorzystanie testów hipotez statystycznych w identyfikacji dryfu modelu w przystępny sposób opisane jest tutaj. "],["demo-detekcja-dryfu.html", "Rozdział 15 Demo: detekcja dryfu", " Rozdział 15 Demo: detekcja dryfu Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 "],["cel-2.html", "15.1 Cel", " 15.1 Cel Celem naszego programu jest detekcja dryfu w oparciu o wyniki ewaluacji z poprzedniego ćwiczenia, zarejestrowane w pliku evaluation/model_eval.csv. Stworzony dzięki temu program będziemy mogli później wykorzystać jako komponent pełnego potoku MLOps. "],["lista-kontrolna-3.html", "15.2 Lista kontrolna", " 15.2 Lista kontrolna Skrypt, który stworzymy, będzie realizował następujące zadania: Wczytanie wyników ewaluacji z pliku evaluation/model_eval.csv. Przygotowanie tych danych do obliczenia testów: “twardego” i parametrycznego Identyfikacja ostatniego odczytu Lista logów miar jakości: RMSE i r2 Przeprowadzenie testów i wydruk ich wyników na ekranie: test “twardy”: Dla RMSE rozpoznajemy dryf (przypisujemy wartość TRUE), jeśli nowe RMSE jest większe od średniej wszystkich poprzednich RMSE Dla r2 identyfikujemy dryf (przypisujemy wartość TRUE), jeśli nowe r2 jest mniejsze od średniej wszystkich poprzednich r2 test parametryczny: Dla RMSE rozpoznajemy dryf (przypisujemy wartość TRUE), jeśli nowe RMSE jest większe od średniej wszystkich poprzednich RMSE + 2*odchylenie standardowe (wszystkich poprzednich RMSE) Dla r2 identyfikujemy dryf (przypisujemy wartość TRUE), jeśli nowe r2 jest mniejsze od średniej wszystkich poprzednich r2 - 2*odchylenie standardowe (wszystkich poprzednich r2). "],["architektura-3.html", "15.3 Architektura", " 15.3 Architektura 15.3.1 Artefakty Wejście Pliku z ewaluacjami: evaluation/model_eval.csv. Wyjście wydruk wyników testów na ekranie. 15.3.2 Komponenty Jeden program 1.detect_model_drift.py realizujący zadania z listy kontrolnej. "],["ćwiczenie-detekcja-dryfu.html", "Rozdział 16 Ćwiczenie: detekcja dryfu", " Rozdział 16 Ćwiczenie: detekcja dryfu Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 W pliku 1.detect_model_drift.py dodaj funkcjonalności umożliwiające: Przeprowadzenie testu nieparametrycznego (IQR) i wydruk jego wyniku na ekranie Dla RMSE rozpoznajemy dryf (przypisujemy wartość TRUE), jeśli nowe RMSE jest większe od trzeciego kwartylu RMSE + 1.5*IQR Dla r2 identyfikujemy dryf (przypisujemy wartość TRUE), jeśli nowe r2 jest mniejsze od pierwszego kwartylu r2 - 1.5*IQR Wygenerowanie sygnału dotrenowania w sytuacji, gdy co najmniej jeden z testów dał wynik pozytywny Zapis sygnału dryfu w przypadku jego zastąpienia do pliku evaluation/model_drift.csv Jeśli ten plik jeszcze nie istnieje: skrypt powinien go stworzyć Jeśli ten plik już istnieje: powinien dodać do niego nowe rekordy Zapisywany rekord powinien zawierać następujące pola: Stempel czasu Wersja modelu Uzasadnienie wygenerowania sygnału dryfu: 6 kolumn (3 testy po dwa parametry) z wartościami TRUE (test pozytywny: sygnał dryfu) lub FALSE (test negatywny: brak sygnału dryfu) Przykładowy kształt pliku: "],["ćwiczenie-uruchomienie-dotrenowania.html", "Rozdział 17 Ćwiczenie: uruchomienie dotrenowania", " Rozdział 17 Ćwiczenie: uruchomienie dotrenowania Uwaga: komplet wersji demonstracyjnych, ćwiczeń i rozwiązań oraz rekomendacje dotyczące środowiska uruchomieniowego znajdziesz tutaj: https://github.com/wodecki/ASI_2022 To proste ćwiczenie zostawiliśmy na koniec: pozwoli one zamknąć pętlę MLOps. Na końcu pliku 1.detect_model_drift.py wykorzystaj bibliotekę Python subprocess do uruchomienia pliku 2. train_model.py w przypadku wystąpienia dryfu. "],["podsumowanie-4.html", "Rozdział 18 Podsumowanie", " Rozdział 18 Podsumowanie image-20220502100431057 Kluczowe etapy w cyklu życia modelu to trenowanie i wykorzystanie produkcyjne (inferencja). Modele nie są doskonałe nie tylko na końcu procesu trenowania: z czasem mogą się one też deaktualizować. Najważniejsze przyczyny dryfu modelu to: Zmiana w zmiennej prognozowanej Zmiana w zmiennych objaśniających (cechach) Zmiany w relacjach cech i zmiennych prognozowanych. Rozwiązaniem jest ciągły monitoring modeli. Najbardziej popularne metody detekcji dryfu to: Test „twardy” (porównawczy) Test istotności parametrycznej Test nieparametryczny (identyfikacja wartości odstających IQR). image-20220502100947866 Detekcja dryfu jest sygnałem konieczności dotrenowania modelu. image-20220502101014133 W efekcie, cykl życia modelu jest sekwencją procesów trenowania i wnioskowania produkcyjnego - aż do momentu wycofania go z użytkowania. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
